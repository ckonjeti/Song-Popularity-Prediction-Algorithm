{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import spotipy\n",
    "import billboard\n",
    "import string\n",
    "#import applemusicpy\n",
    "import lyricsgenius\n",
    "#import SpotifyCharts as sCharts\n",
    "import csv\n",
    "import os\n",
    "from textstat.textstat import textstat\n",
    "import threading as thread\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "from sklearn import linear_model, mixture\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Chaitu Konjeti/SongPopularityPredictionAlgorithm/output\"\n",
    "genius = lyricsgenius.Genius(\"Xf0XfBJTZon0Sra2rGV56TAXp6jOUaLJVhmHxqbTW5mp-j6S2NVcmHWSLQ29v0dk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeSongCharacteristics(i, billboardChart, songs):\n",
    "    key = str(billboardChart[i].title + billboardChart[i].artist)\n",
    "\n",
    "    if key not in songs:\n",
    "        song = billboardChart[i]\n",
    "        title = billboardChart[i].title\n",
    "        artist = song.artist\n",
    "        peakpos = song.peakPos\n",
    "        lastpos = song.lastPos\n",
    "        numWeeks = song.weeks\n",
    "        currentPos = song.rank\n",
    "        isNew = song.isNew\n",
    "        lyrics = genius.search_song(title, artist).lyrics\n",
    "        row = [title, artist, lyrics, peakpos, lastpos, numWeeks, currentPos, isNew]\n",
    "        songs[key] = row\n",
    "\n",
    "    if key in songs:\n",
    "        if billboardChart[i].peakPos != songs.get(key)[3]:\n",
    "            newRow = songs.get(key)\n",
    "            newRow[3] = billboardChart[i].peakPos\n",
    "            songs[key] = newRow\n",
    "        if billboardChart[i].weeks != songs.get(key)[5]:\n",
    "            newRow = songs.get(key)\n",
    "            newRow[5] = billboardChart[i].weeks\n",
    "            songs[key] = newRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllSongData(startMonth, startYear, endMonth, endYear, numSongs):\n",
    "    for year in range(startYear, endYear + 1):\n",
    "        thread.Thread(None, target=yearlySongData, args=(year, startMonth, endMonth, numSongs)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearlySongData(year, startMonth, endMonth, numSongs):\n",
    "    for month in range (startMonth, endMonth + 1):\n",
    "            thread.Thread(None, target=monthlySongData, args=(year, month, numSongs)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthlySongData(year, month, numSongs):\n",
    "        outputFileName = \"C:/Users/Chaitu Konjeti/SongPopularityPredictionAlgorithm/output/billboardHot100_Lyrics_{}_{}.csv\".format(year, month)\n",
    "\n",
    "        with open(outputFileName, 'a+', newline='', encoding='utf-8') as outputFile:\n",
    "            songs = {}\n",
    "            day = 1\n",
    "            dataWriter = csv.writer(outputFile)\n",
    "\n",
    "            while (day <= 31):\n",
    "                if month in range(1, 10):\n",
    "                    try:\n",
    "                        billboardChart = billboard.ChartData('hot-100', date = \"{}-{:02d}-{}\".format(year, month, day))\n",
    "                        for i in range(0, numSongs):\n",
    "                            try:\n",
    "                                writeSongCharacteristics(i, billboardChart, songs)\n",
    "                            except:\n",
    "                                pass\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif month in range(10, 13):\n",
    "                    try:\n",
    "                        billboardChart = billboard.ChartData('hot-100', date = \"{}-{}-{}\".format(year, month, day))\n",
    "                        for i in range(0, numSongs):\n",
    "                            try:\n",
    "                                writeSongCharacteristics(i, billboardChart, songs)\n",
    "                            except:\n",
    "                                pass\n",
    "                    except:\n",
    "                        pass\n",
    "                day += 5\n",
    "\n",
    "            for key in songs:\n",
    "                if (len(songs[key][2]) < 10000):\n",
    "                    dataWriter.writerow(songs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyric_preprocessing(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        data = []\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        with open(dir + filename, encoding='utf-8', mode='r+') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            for row in csv_reader:\n",
    "                temp = row\n",
    "                num_positive = 0\n",
    "                num_negative = 0\n",
    "                num_neutral = 0\n",
    "                dale_chall_score = 0\n",
    "                #print(row[0])\n",
    "                lyrics = row[2]\n",
    "                lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics)\n",
    "                lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "                #print(lyrics)\n",
    "                #this_sentence = lyrics.decode('utf-8')\n",
    "                #numlines = 0\n",
    "                for line in lyrics.splitlines():\n",
    "                    #print(line)\n",
    "                    comp = sid.polarity_scores(line)\n",
    "                    #print(comp)\n",
    "                    comp = comp['compound']\n",
    "                    if comp >= 0.5:\n",
    "                        num_positive += 1\n",
    "                    elif comp > -0.5 and comp < 0.5:\n",
    "                        num_neutral += 1\n",
    "                    else:\n",
    "                        num_negative += 1\n",
    "\n",
    "                    #numlines += 1\n",
    "\n",
    "                temp.append(num_positive)\n",
    "                temp.append(num_neutral)\n",
    "                temp.append(num_negative)\n",
    "                temp.append(textstat.dale_chall_readability_score(lyrics))\n",
    "                temp.append(textstat.difficult_words(lyrics))\n",
    "                temp.append(lyrics)\n",
    "                data.append(temp)\n",
    "                #print(temp)\n",
    "            outputFileName = \"C:/Users/Chaitu Konjeti/song-popularity-prediction-algorithm/processed/\" + filename\n",
    "\n",
    "            with open(outputFileName, 'a+', newline='', encoding='utf-8') as outputFile:\n",
    "                dataWriter = csv.writer(outputFile)\n",
    "                for items in data:\n",
    "                    #print(items)\n",
    "                    dataWriter.writerow(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getAllSongData(1, 2000, 12, 2019, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-7a8d0176b64d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlyric_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-1a6c3444b2d1>\u001b[0m in \u001b[0;36mlyric_preprocessing\u001b[1;34m(dir)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlyrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[1;31m#print(line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0mcomp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                     \u001b[1;31m#print(comp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[0mcomp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mvalence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \"\"\"\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[0msentitext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;31m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words_and_emoticons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;31m# doesn't separate words from\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;31m# adjacent punctuation (keeps emoticons & contractions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m_words_and_emoticons\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mwes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mwords_punc_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words_plus_punc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m         \u001b[0mwes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwe\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwes\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m_words_plus_punc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# the product gives ('cat', ',') and (',', 'cat')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mpunc_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mpunc_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPUNC_LIST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpunc_before\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpunc_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# the product gives ('cat', ',') and (',', 'cat')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mpunc_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPUNC_LIST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mpunc_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPUNC_LIST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpunc_before\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mwords_punc_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpunc_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lyric_preprocessing(\"output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'C:/Users/Chaitu Konjeti/song-popularity-prediction-algorithm/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(dir):\n",
    "    with open(dir + filename, encoding='utf-8-sig', mode='r') as csv_file:\n",
    "        df = pd.read_csv(dir + filename, header = None)\n",
    "        #print(df.head())\n",
    "        df.columns = ['title', 'artist', 'original', 'peakpos', 'lastpos', 'numWeeks', 'currentPos', 'isNew', 'pos', 'neu', 'neg', 'dale', 'diff', 'lyrics']\n",
    "        for lyric in df['lyrics']:\n",
    "            X.append(lyric)\n",
    "        for pos in df['currentPos']:\n",
    "            y.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1483"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for post in X:\n",
    "    tokens += word_tokenize(post)\n",
    "stop_words = [\"I\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13716\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(tokens)\n",
    "# print(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dist = nltk.FreqDist(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1483,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_~\\t\\n',\n",
    "                      lower = True, split = ' ')\n",
    "\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "sequences = np.asarray(sequences)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "for i in range(len(sequences)):\n",
    "    label = y[i]\n",
    "    #print(label)\n",
    "    for j in range(0, len(sequences[i]), 50):\n",
    "        len_of_50 = sequences[i][j:j + 50]\n",
    "        if len(len_of_50) == 50:\n",
    "            features.append(sequences[i][j:j + 50])\n",
    "            labels.append(label)\n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12264, 50)\n",
      "(12264,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 382,    3,   31, ...,   75,   87,   26],\n",
       "       [   2,    7,   98, ...,   69,  109,   90],\n",
       "       [   3,   20,  452, ...,   51,   48,   11],\n",
       "       ...,\n",
       "       [ 263, 1596, 2632, ...,    3, 5651,   30],\n",
       "       [  14,  112,   27, ...,    3, 2156,  589],\n",
       "       [  49,   49,  589, ..., 1023,    1,  618]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "X = count_vectorizer.fit_transform(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  18   43    9 ...   40  746    9]\n",
      " [  24 4609   28 ...  117  159  105]\n",
      " [   2 1299 1871 ...  890   57    2]\n",
      " ...\n",
      " [  34  689  140 ... 3031 2683 1010]\n",
      " [ 168    3  214 ... 1963   45   83]\n",
      " [  67   45   10 ... 4237   13   16]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=1)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 1000)        20000000  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 600)               3842400   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 101)               30401     \n",
      "=================================================================\n",
      "Total params: 24,053,101\n",
      "Trainable params: 24,053,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=20000,\n",
    "        output_dim=1000,\n",
    "        weights=None,\n",
    "        trainable=True))\n",
    "\n",
    "# Recurrent layer\"\n",
    "model.add(\n",
    "    LSTM(\n",
    "        600, return_sequences=False, dropout=0.1,\n",
    "        recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(300, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(101, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9811/9811 [==============================] - 134s 14ms/step - loss: 4.6115 - accuracy: 0.0158\n",
      "Epoch 2/30\n",
      "9811/9811 [==============================] - 128s 13ms/step - loss: 4.5820 - accuracy: 0.0192\n",
      "Epoch 3/30\n",
      "9811/9811 [==============================] - 129s 13ms/step - loss: 4.5797 - accuracy: 0.0361\n",
      "Epoch 4/30\n",
      "9811/9811 [==============================] - 130s 13ms/step - loss: 4.5134 - accuracy: 0.0387\n",
      "Epoch 5/30\n",
      "9811/9811 [==============================] - 128s 13ms/step - loss: 4.4122 - accuracy: 0.0434\n",
      "Epoch 6/30\n",
      "9811/9811 [==============================] - 130s 13ms/step - loss: 4.3384 - accuracy: 0.0614\n",
      "Epoch 7/30\n",
      "9811/9811 [==============================] - 130s 13ms/step - loss: 4.1741 - accuracy: 0.0761\n",
      "Epoch 8/30\n",
      "9811/9811 [==============================] - 130s 13ms/step - loss: 3.9582 - accuracy: 0.0985\n",
      "Epoch 9/30\n",
      "9811/9811 [==============================] - 132s 13ms/step - loss: 3.7429 - accuracy: 0.1300\n",
      "Epoch 10/30\n",
      "9811/9811 [==============================] - 130s 13ms/step - loss: 3.5063 - accuracy: 0.1651\n",
      "Epoch 11/30\n",
      "9811/9811 [==============================] - 155s 16ms/step - loss: 3.2535 - accuracy: 0.2063\n",
      "Epoch 12/30\n",
      "9811/9811 [==============================] - 138s 14ms/step - loss: 3.0402 - accuracy: 0.2437\n",
      "Epoch 13/30\n",
      "9811/9811 [==============================] - 136s 14ms/step - loss: 2.8893 - accuracy: 0.2704\n",
      "Epoch 14/30\n",
      "9811/9811 [==============================] - 143s 15ms/step - loss: 2.7361 - accuracy: 0.2992\n",
      "Epoch 15/30\n",
      "9811/9811 [==============================] - 151s 15ms/step - loss: 2.5565 - accuracy: 0.3308\n",
      "Epoch 16/30\n",
      "9811/9811 [==============================] - 151s 15ms/step - loss: 2.4394 - accuracy: 0.3501\n",
      "Epoch 17/30\n",
      "9811/9811 [==============================] - 146s 15ms/step - loss: 2.3113 - accuracy: 0.3682\n",
      "Epoch 18/30\n",
      "9811/9811 [==============================] - 157s 16ms/step - loss: 2.2193 - accuracy: 0.3781\n",
      "Epoch 19/30\n",
      "9811/9811 [==============================] - 149s 15ms/step - loss: 2.1261 - accuracy: 0.3917\n",
      "Epoch 20/30\n",
      "9811/9811 [==============================] - 158s 16ms/step - loss: 2.0574 - accuracy: 0.3993\n",
      "Epoch 21/30\n",
      "9811/9811 [==============================] - 173s 18ms/step - loss: 1.9826 - accuracy: 0.4084\n",
      "Epoch 22/30\n",
      "9811/9811 [==============================] - 146s 15ms/step - loss: 1.9220 - accuracy: 0.4180\n",
      "Epoch 23/30\n",
      "9811/9811 [==============================] - 152s 15ms/step - loss: 1.8719 - accuracy: 0.4243\n",
      "Epoch 24/30\n",
      "9811/9811 [==============================] - 167s 17ms/step - loss: 1.8250 - accuracy: 0.4244\n",
      "Epoch 25/30\n",
      "9811/9811 [==============================] - 142s 15ms/step - loss: 1.7938 - accuracy: 0.4280\n",
      "Epoch 26/30\n",
      "9811/9811 [==============================] - 143s 15ms/step - loss: 1.7496 - accuracy: 0.4205\n",
      "Epoch 27/30\n",
      "9811/9811 [==============================] - 144s 15ms/step - loss: 1.7191 - accuracy: 0.4290\n",
      "Epoch 28/30\n",
      "9811/9811 [==============================] - 159s 16ms/step - loss: 1.6850 - accuracy: 0.4355\n",
      "Epoch 29/30\n",
      "9811/9811 [==============================] - 144s 15ms/step - loss: 1.6674 - accuracy: 0.4346\n",
      "Epoch 30/30\n",
      "9811/9811 [==============================] - 133s 14ms/step - loss: 1.6410 - accuracy: 0.4300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x222a566b048>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, \n",
    "        batch_size=1500, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0745322915169"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred = model.predict(X_test)\n",
    "nn_pred = np.argmax(nn_pred, axis=1)\n",
    "smape(nn_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 84, 44, ..., 28, 99, 47])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042605239017429415"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  3, 11, ..., 62, 57, 47])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 84, 44, ..., 28, 99, 47])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.41495519101447"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smape(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019160211985324093"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitu Konjeti\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4517378452757109"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rff_clf = RandomForestClassifier(max_depth=30)\n",
    "rff_clf.fit(X_train, y_train)\n",
    "rff_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 86,  3, ..., 47, 98, 47])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = (rff_clf.predict(X_test))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.237533765059645"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smape(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09539339584182634"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitu Konjeti\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Chaitu Konjeti\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Chaitu Konjeti\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05126898379370095"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr_clf = LogisticRegression()\n",
    "logr_clf.fit(X_train, y_train)\n",
    "logr_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logr_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-934ee8116245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogr_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logr_clf' is not defined"
     ]
    }
   ],
   "source": [
    "pred = logr_clf.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.237533765059645"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smape(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
